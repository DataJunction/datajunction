- [X] Single model for all nodes
- [X] All nodes in the same directory
- [X] Compile repo, computing the schema of source nodes
- [X] API for running sync queries
- [X] API for running async queries
- [X] Results backend
- [X] Celery workers for async queries
- [X] Add DJ and Redis to docker-compose
- [X] Queries with multiple statements
- [X] Return statement SQL with `results`
- [X] Add Celery to docker-compose
- [X] Organize files (`api/`, `models/`, etc)
- [/] Compute the schema of downstream nodes
- [ ] Write columns back to YAML
- [ ] Representations should have columns
- [ ] Paginating results
- [ ] Limit results
- [ ] Optimize data transfer (delta-of-delta for timeseries, msgpack?)
- [ ] Translate metrics into SQL
- [ ] Compute metrics (ie, run query)
- [ ] Compute statistics on columns (histogram, correlation)
- [ ] Move data on JOINs based on column statistics
- [ ] Virtual dimensions (time, space, user-defined)
- [ ] JS dataframe with time-aware caching and additive-aware, to reuse queries
- [ ] 2 modes of join: Shillelagh and move data
- [ ] Auto-map dimensions from the DB schema?
- [ ] UUID for models?

Node types:

- source
- transform
- dimension
- metric
- population

Relationships:

- source -> transform [-> transform ]-> metric
- source -> dimension -> population
- population based on metrics as well?

DSL for querying:

- `m=likes,comments&d=user.country&f=userid>10,time>2021-01-01T00:00:00+00:00`

Examples:

1. Dimension table in 2 storages (fast/slow), choose fast.
2. Dimension table in 2 storages (fast/slow) but only a few columns in fast; choose slow.
3. Translate query with cross-DB join, `FROM PROGRAM` in Postgres

SQL input:

```
SELECT core.likes FROM metrics
```

Gets translated to:

```
SELECT COUNT(*) FROM content_actions
```
